\documentclass{article}
\usepackage{titlesec} %for styling of sections
\usepackage{dsfont} %for \mathds{1}
\usepackage{amsfonts} %for \mathbb{R}
\usepackage{amsmath} % for \begin{align}
\usepackage{hyperref} %for url references
\usepackage[margin=0.5in]{geometry} %for narrow margins
\usepackage{amsthm}
% --------------------------------------------------
% for increasing the space before and after equation
\makeatletter
\g@addto@macro\normalsize{
  \setlength\abovedisplayskip{10pt}
  \setlength\belowdisplayskip{10pt}
  \setlength\abovedisplayshortskip{10pt}
  \setlength\belowdisplayshortskip{10pt}
}
% --------------------------------------------------
\makeatother
\newtheorem{definition}{Definition}
\newtheorem{assumption}{Assumption}
\newtheorem{example}{Example}[section]

\titleformat{\section}
    {\normalfont\Large\bfseries}
    {\thesection}
    {20pt}
    {\Large}
    [{\titlerule[0.8pt]}]

\titleformat{\subsection}
    {\normalfont\Large\bfseries}
    {\thesubsection}
    {20pt}
    {\Large}
    %[{\titlerule}]
    [\vskip-10pt{\makebox[\linewidth][l]{\rule{0.75\textwidth}{0.5pt}}}]

\titleformat{\subsubsection}
    {\normalfont\large\bfseries}
    {\thesubsubsection}
    {20pt}
    {\large}
    %[\vskip-10pt{\makebox[\linewidth][l]{\rule{0.5\textwidth}{0.5pt}}}]


\title{Chi-square test}
\author{Grzegorz Karas}

\begin{document}

\maketitle

%\tableofcontents

\section{Definition \cite{wiki, paper}}
The $\chi^2$-test is a statistical method to test hypothesis, where random variable
follows multinomial distribution. It tests a null hypothesis stating that the frequency distribution of certain 
events observed in a sample is consistent with a particular theoretical distribution.

The most common $\chi^2$-test is a Pearson's chi-square test in which the test statistic is of following kind
\begin{equation}
    \chi^2=\sum_{i=1}^n \frac{\left(O_i-E_i\right)^2}{E_i} = N \sum_{i=1}^n \frac{\left(O_i/N-p_i\right)^2}{p_i}
\end{equation}
where
\begin{itemize}
    \item $\chi^2$ - Pearson's cumulative test statistic, which asymptotically approaches a $\chi^2$ distribution.
    \item $O_i$ - number of observation of type i
    \item N - total number of observations
    \item $E_i = Np_i$ - the expected (theoretical) count of type i
    \item n - the number of cells in the table ($rows\cdot columns$)
\end{itemize}

If the test fails, then the appropriate test statistic has approximately 
a noncentral $\chi^2$ distribution with the same degrees of freedom ($df$) and
a noncentrality parameter $\lambda$, which depends on alternative considered.

\section{Test types \cite{wiki}}
Pearson's chi-squared test is used to assess three types of hypothesis testing: 
goodness of fit, homogeneity, and independence.

\begin{itemize}
    \item A test of \textbf{goodness of fit} establishes whether an observed frequency 
    distribution differs from a theoretical distribution.
    
    The $\chi^2$ test statistic follows the $\chi^2$ distribution with $df=n-1$.
    \item A test of \textbf{homogeneity} compares the distribution of counts for 
    two or more groups using the same categorical variable.

    The $\chi^2$ test statistic follows the $\chi^2$ distribution with $df=(rows-1)\cdot(columns-1)$.
    \item A test of \textbf{independence} assesses whether observations consisting of 
    measures on two variables, expressed in a contingency table, are independent of each other.
    
    The $\chi^2$ test statistic follows the $\chi^2$ distribution with $df=(rows-1)\cdot(columns-1)$.
\end{itemize}

\subsection{Test of goodness of fit}

Let $X=(X_1, ..., X_k)$ be a multinomial random variable with parameters
$n, p_1, ..., p_k$. Suppose we wish to test
\begin{equation}\label{goodness_of_fit_h_0}
    H_0: \quad p_i=p_{i}^0 \qquad i =1,2,...,k
\end{equation}
against
\begin{equation}
    H_a: \mbox{not all p's are as given by } H_0
\end{equation}
where the $p_{i}'$ are given expected numbers. The value of the chi-square test-statistic is
\begin{equation}
    \chi^2_{H_0} = \sum_{i=1}^k \frac{\left(x_i - np_{i}^0\right)^2}{np_{i}^0} = \sum_{i=1}^k \frac{\left(\widehat{p_i}  - p_{i}^0\right)^2}{p_{i}^0}
\end{equation}
The chi-square test reject $H_0$ if
\begin{equation}
    \chi^2_{H_0} > \chi^2_{k-1,1-\alpha},
\end{equation}
where $\alpha$ is the significance level, and $\chi^2_{k-1,1-\alpha}$ is the 
quantile of order $1-\alpha$ of the central $\chi^2$ distribution with $k-1$ degrees of
freedom.
The p-value of the test is
\begin{equation}
    p-value = P\left(X>\chi^2_{H_0}\right)
\end{equation}
To evaluate the power of the test let's precisely define the alternative $H_a$ as follow.
\begin{equation}\label{goodness_of_fit_h_a}
    H_a: \quad p_i=p_{i}^a \qquad i =1,2,...,k
\end{equation}
Thus the power o the test is
\begin{equation}
    Power = P^{\lambda}\left(X > \chi^2_{k-1,1-\alpha}\right)
\end{equation}
where $X$ is a ranom variable that follows the noncentral $\chi^2$ distribution with the noncentrality parameter
\begin{equation}\label{goodness_of_fit_lambda}
    \lambda = n \sum_{i=1}^k \frac{\left(p_{i}^a-p_{i}^0\right)^2}{p_{i}^0}
\end{equation}




\subsection{Test of independence}
Let $X=(X_{ij}) \in \mathbb{R}^{r \times c}$ be a multinomial random variable with
parameters $n,p_{ij}$ where $i=1,2,...,r$, $j=1,2,...,c$ and 
$\sum_{i=1}^r\sum_{j=1}^cp_{ij}=1$. 
Suppose we wish to test independence
\begin{equation}
    H_0: \quad p_{ij}=p_{i \cdot}p_{\cdot j} \qquad i =1,2,...,r \quad j=1,2,...,c
\end{equation}
against
\begin{equation}
    H_a: \mbox{not all the equatons given under $H_0$ are satisfied}
\end{equation}
where $p_{i \cdot} = \sum_{j=1}^c p_{ij}$ and $p_{\cdot j} = \sum_{i=1}^r p_{ij}$.
The value of the chi-square test-statistic is
\begin{equation}
    \chi^2_{H_0} = \sum_{i=1}^r\sum_{j=1}^c \frac{\left(x_{ij} - x_{i \cdot}x_{\cdot j}/n\right)^2}{x_{i \cdot}x_{\cdot j}/n}
\end{equation}
where $x_{i \cdot} = \sum_{j=1}^c x_{ij}$ and $x_{\cdot j} = \sum_{i=1}^r x_{ij}$.
We observe that
\begin{align}
    \chi^2_{H_0} =\frac{1}{n} \sum_{i=1}^r\sum_{j=1}^c \frac{\left(\frac{x_{ij}}{n} - \frac{x_{i \cdot}}{n}\frac{x_{\cdot j}}{n}\right)^2}{\frac{x_{i \cdot}}{n}\frac{x_{\cdot j}}{n}}=  \frac{1}{n} \sum_{i=1}^r\sum_{j=1}^c \frac{\left(\widehat{p_{ij}} - \widehat{p_{i \cdot}}\widehat{p_{\cdot j}}\right)^2}{\widehat{p_{i \cdot}}\widehat{p_{\cdot j}}}
\end{align}
The chi-square test reject $H_0$ if
\begin{equation}
    \chi^2_{H_0} > \chi^2_{(r-1)\cdot(c-1),1-\alpha},
\end{equation}
To evaluate the power of the test let's precisely define the alternative $H_a$ as follow.
\begin{equation}
    H_a: \quad p_{ij} =\underbrace{p_{i\cdot}p_{\cdot j} + \frac{c_{ij}}{\sqrt{n}}}_{p^a_{ij}}, \qquad i =1,2,...,r \quad j=1,2,...,c, \quad where \quad \sum_{i=1}^{r}\sum_{j=1}^{c}c_{ij}=0, 
\end{equation}
Thus the power of the test is
\begin{equation}
    Power = P^{\lambda}\left(X > \chi^2_{(r-1)\cdot(c-1),1-\alpha}\right)
\end{equation}
where $X$ is a random variable that follows the noncentral $\chi^2$ distribution with the noncentrality parameter
\begin{equation}
    \lambda = \sum_{i=1}^{r}\sum_{j=1}^c \frac{c_{ij}^2}{p_{i\cdot}p_{\cdot j}} - \sum_{i=1}^{r}\frac{c_{i \cdot}^2}{p_{i \cdot}} - \sum_{j=1}^{c}\frac{c_{\cdot j}^2}{p_{\cdot j}},
\end{equation}
where $c_{i \cdot} = \sum_{j=1}^c c_{ij}$ and $c_{\cdot j} = \sum_{i=1}^r c_{ij}$.

If $\Delta_{ij} = c_{ij}/\sqrt{n}$, then 
\begin{equation}\label{independence_lambda}
    \lambda = n\left[\sum_{i=1}^{r}\sum_{j=1}^c \frac{\Delta_{ij}^2}{p_{i\cdot}p_{\cdot j}} - \sum_{i=1}^{r}\frac{\Delta_{i \cdot}^2}{p_{i \cdot}} - \sum_{j=1}^{c}\frac{\Delta_{\cdot j}^2}{p_{\cdot j}}\right],
\end{equation}



\subsection{Test of homogeneity}
Let $X_i=(X_{ij}) \in \mathbb{R}^{c}$ be a multinomial random variable with
parameters $n_{i},p_{ij}$ for $i=1,2,...,r$ and $\sum_{j=1}^cp_{ij}=1$. 
Suppose we wish to test homogeneity
\begin{equation}
    H_0: \quad p_{1j}=p_{2j}=\cdots = p_{rj} = p_{\cdot j}\quad j=1,2,...,c
\end{equation}
against
\begin{equation}
    H_a: \mbox{not all the equatons given under $H_0$ are satisfied}
\end{equation}
The value of a chi-square test-statistic is
\begin{equation}
    \chi^2_{H_0} = \sum_{i=1}^r\sum_{j=1}^c \frac{\left(x_{ij} - x_{i \cdot}x_{\cdot j}/n\right)^2}{x_{i \cdot}x_{\cdot j}/n}
\end{equation}
where $x_{i \cdot} = \sum_{j=1}^c x_{ij} = n_{i}$ and $n = \sum_{i=1}^r n_{i}$. Please be aware that the parameter $n_i$ is selected before an experiment starts. 
In other words there is no randomness in total observations of each random variables $X_1$, ... $X_i$. 
In the test of independence the count of observations in each group has not been defined before the experiment and only the total number of observations for all groups was given.
This is because the split between groups in the test of independence is a part of random variable whereas the split between groups in the test of homogenity is deterministic.
The chi-square test reject $H_0$ if
\begin{equation}
    \chi^2_{H_0} > \chi^2_{(r-1)\cdot(c-1),1-\alpha},
\end{equation}
To evaluate the power of the test let's precisely define the alternative $H_a$ as follow.
\begin{equation}
    H_a: p_{ij} = \underbrace{p_{\cdot j} + \frac{c_{ij}}{\sqrt{n}}}_{p^a_{ij}} , \qquad j=1,2,...,c \quad where \quad \sum_{j=1}^{c}c_{ij}=0, 
\end{equation}
Thus the power of the test is
\begin{equation}
    Power = P^{\lambda}\left(X_a > \chi^2_{(r-1)\cdot(c-1),1-\alpha}\right)
\end{equation}
where $X_a$ is a random variable that follows the noncentral $\chi^2$ distribution with the noncentrality parameter
\begin{equation}
    \lambda = \sum_{j=1}^{c}\frac{1}{p_{\cdot j}}\left[ \sum_{i=1}^{r} c_{ij}^2 \frac{n_i}{n} -  \left(\sum_{i=1}^{r} c_{ij} \frac{n_i}{n}\right)^2  \right],
\end{equation}
If $\Delta_{ij} = c_{ij}/\sqrt{n}$, then 
\begin{equation}
    \lambda = n\sum_{j=1}^{c}\frac{1}{p_{\cdot j}}\left[ \sum_{i=1}^{r} \Delta_{ij}^2 \frac{n_i}{n} -  \left(\sum_{i=1}^{r} \Delta_{ij} \frac{n_i}{n}\right)^2  \right]
\end{equation}
It is worth to observe that $\frac{n_i}{n}$ is the same as $p_{i\cdot}$ and the equation holds following form
\begin{equation}\label{homogeneity_lambda}
    \lambda = n\sum_{j=1}^{c}\frac{1}{p_{\cdot j}}\left[ \sum_{i=1}^{r} \Delta_{ij}^2 p_{i\cdot} -  \left(\sum_{i=1}^{r} \Delta_{ij} p_{i\cdot}\right)^2  \right]
\end{equation}

\section{Sample size}
The sample size required for a test to reach predefined power can be calculated
under following assumption 

\begin{assumption}
The alternative hypothesis is the one given by the observed sample. 
\end{assumption} 
In other words observed estimates construct the alternative hypothesis. The 
procedure to retrieve the sample size is following:

\begin{enumerate}
    \item Calculate contingency table in terms of probabilities ($p^a_{ij}$).
    \item For goodness-of-fit set the probabilities, for other tests calculate expected values from the contingency table ($\widehat{p_{ij}}$).
    \item Calculate deltas $\Delta_{ij}$ between contingency table and values from the set or calculated expected values from the previous step.
    \item Calculate the $\alpha$-quantile $\chi^2_{df,1-\alpha}$ of a central chi-square distribution.
    \item Having defined target power ($\beta$) of a test, find the noncentrality parameter ($\lambda$) of a noncentral chi-square distribution. 
    The parameter $\lambda$ is found solving following equation
    \begin{equation}
        \beta = P^\lambda\left(X > \chi^2_{df,1-\alpha} \right).
    \end{equation}
    \item Depending on the type of test calculate the sample size $n$. Details of the calculation is shown in the next sections.
\end{enumerate}


\subsection{Sample size - test of goodness of fit}
The $p_i^0$ from Equation \ref{goodness_of_fit_h_0} is defined a priori for every $i$.
The $p_i^a$ from Equation \ref{goodness_of_fit_h_a} is defined a posteriori and is equal to the estimate $\widehat{p_{i}}$ derived from observed sample for every $i$.
In this case the Equation \ref{goodness_of_fit_lambda} is following:

\begin{equation}
    \lambda = n \sum_{i=1}^k \frac{\left(\widehat{p_{i}}-p_{i}^0\right)^2}{p_{i}^0}
\end{equation}
So
\begin{equation}
    n = \frac{\lambda}{\sum_{i=1}^k \frac{\left(\widehat{p_{i}}-p_{i}^0\right)^2}{p_{i}^0}}
\end{equation}


\subsection{Sample size - test of independence}

Having performed steps until 5 the Equation \ref{independence_lambda} is following

\begin{equation}
    \lambda = n\left[\sum_{i=1}^{r}\sum_{j=1}^c \frac{\Delta_{ij}^2}{\widehat{p_{i\cdot}}\widehat{p_{\cdot j}}} - \sum_{i=1}^{r}\frac{\Delta_{i \cdot}^2}{\widehat{p_{i \cdot}}} - \sum_{j=1}^{c}\frac{\Delta_{\cdot j}^2}{\widehat{p_{\cdot j}}}\right],
\end{equation}
So the sample sie $n$ is
\begin{equation}
    n = \frac{\lambda}{\left[\sum_{i=1}^{r}\sum_{j=1}^c \frac{\Delta_{ij}^2}{\widehat{p_{i\cdot}}\widehat{p_{\cdot j}}} - \sum_{i=1}^{r}\frac{\Delta_{i \cdot}^2}{\widehat{p_{i \cdot}}} - \sum_{j=1}^{c}\frac{\Delta_{\cdot j}^2}{\widehat{p_{\cdot j}}}\right]},
\end{equation}

\subsection{Sample size - test of homogeneity}

Having performed steps until 5 the Equation \ref{homogeneity_lambda} is following

\begin{equation}
    \lambda = n\sum_{j=1}^{c}\frac{1}{\widehat{p_{\cdot j}}}\left[ \sum_{i=1}^{r} \Delta_{ij}^2 p_{i\cdot} -  \left(\sum_{i=1}^{r} \Delta_{ij} p_{i\cdot}\right)^2  \right]
\end{equation}
So the sample sie $n$ is
\begin{equation}
    n = \frac{\lambda}{\sum_{j=1}^{c}\frac{1}{\widehat{p_{\cdot j}}}\left[ \sum_{i=1}^{r} \Delta_{ij}^2 p_{i\cdot} -  \left(\sum_{i=1}^{r} \Delta_{ij} p_{i\cdot}\right)^2  \right]}
\end{equation}

\begin{thebibliography}{9}
    \bibitem{wiki} 
    Chi-squared-test 
    \url{https://en.wikipedia.org/wiki/Pearson\%27s\_chi-squared\_test}.
    \bibitem{paper} 
    Guenther, W. (1977). 
    \textit{Power and Sample Size for Approximate Chi-Square Tests.} 
    The American Statistician, 31(2), 83-85.

\end{thebibliography}   

\end{document}